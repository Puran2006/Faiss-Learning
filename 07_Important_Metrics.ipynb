{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "add684c9",
   "metadata": {},
   "source": [
    "# recall@k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "566d9216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(actual, predicted, k):\n",
    "    actual_set = set(actual)\n",
    "    predicted_set = set(predicted[:k])\n",
    "    intersection = actual_set.intersection(predicted_set)\n",
    "    recall_value = len(intersection) / len(actual_set)\n",
    "    return recall_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abe1ed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall at 1: 0.00\n",
      "Recall at 2: 0.17\n",
      "Recall at 3: 0.33\n",
      "Recall at 4: 0.50\n",
      "Recall at 5: 0.67\n",
      "Recall at 6: 0.67\n",
      "Recall at 7: 0.67\n",
      "Recall at 8: 0.83\n",
      "Recall at 9: 0.83\n"
     ]
    }
   ],
   "source": [
    "actual = [2, 4, 5, 7, 9, 3]\n",
    "predicted = [1, 2, 3, 4, 5, 6, 8, 9, 10]\n",
    "# k is how many top elements to consider from predicted list\n",
    "k = 9\n",
    "for i in range(1, k+1):\n",
    "    rec = recall(actual, predicted, i)\n",
    "    print(f\"Recall at {i}: {rec:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13faf8be",
   "metadata": {},
   "source": [
    "# MRR ( Mean Reciprocal Rank)\n",
    "Mean Reciprocal Rank (MRR) is a metric used to evaluate ranking systems, particularly in information retrieval. Here are 5 key points about MRR:\n",
    "\n",
    "1. **Definition**: MRR measures the average of the reciprocal ranks of the first relevant item for a set of queries.\n",
    "\n",
    "2. **Relevance Focus**: Unlike Recall@k which considers all relevant items, MRR emphasizes finding the first relevant result quickly.\n",
    "\n",
    "3. **Range**: MRR values range from 0 to 1, where 1 indicates the first item is always relevant, and lower values indicate relevant items appear later in rankings.\n",
    "\n",
    "4. **Use Cases**: MRR is particularly useful for search engines and question-answering systems where users typically focus on the top result.\n",
    "\n",
    "5. **Sensitivity to Position**: MRR heavily penalizes systems where relevant items appear far down in the ranking, making it stricter than some other metrics.\n",
    "\n",
    "**Formula:**\n",
    "$$MRR = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} \\frac{1}{rank_q}$$\n",
    "\n",
    "Where:\n",
    "- |Q| = number of queries\n",
    "- rank_q = the rank of the first relevant item for query q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f19e253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1:  Reciprocal: 0.50\n",
      "Query 2:  Reciprocal: 1.00\n",
      "Query 3:  Reciprocal: 0.10\n",
      "Mean Reciprocal Rank (MRR): 0.53\n"
     ]
    }
   ],
   "source": [
    "# actual relevant results for three queries\n",
    "actual_results = [\n",
    "    [2, 4, 5, 7, 9, 3],\n",
    "    [1, 3, 6, 8],\n",
    "    [10, 11, 12, 13, 14]\n",
    "]\n",
    "# Number of queries\n",
    "Q = len(actual_results)\n",
    "\n",
    "reciprocal = 0\n",
    "for  i in range( Q ):\n",
    "    first_result_relavent = actual_results[i][0]\n",
    "    reciprocal += 1 / ( first_result_relavent )\n",
    "    print(f\"Query {i+1}:  Reciprocal: {1 / first_result_relavent:.2f}\")\n",
    "\n",
    "mrr = reciprocal / Q\n",
    "print(f\"Mean Reciprocal Rank (MRR): {mrr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80308f4f",
   "metadata": {},
   "source": [
    "# MAP(Mean Average Precision)\n",
    "Mean Average Precision (MAP) is a metric that evaluates both the relevance and ranking quality of results. Here are 3 key points:\n",
    "\n",
    "1. **Definition**: MAP calculates the average precision across multiple queries, where precision is measured at each position where a relevant item appears.\n",
    "\n",
    "2. **Combines Precision and Recall**: Unlike MRR which only considers the first relevant item, MAP considers all relevant items and their positions in the ranking.\n",
    "\n",
    "3. **Range and Interpretation**: MAP values range from 0 to 1, where higher values indicate better ranking quality with relevant items appearing earlier.\n",
    "\n",
    "**Formula:**\n",
    "$$MAP = \\frac{1}{|Q|} \\sum_{q=1}^{|Q|} AP(q)$$\n",
    "\n",
    "$$AP(q) = \\frac{1}{|R_q|} \\sum_{k=1}^{n} P(k) \\times rel(k)$$\n",
    "\n",
    "Where:\n",
    "- |Q| = number of queries\n",
    "- AP(q) = Average Precision for query q\n",
    "- |R_q| = number of relevant items for query q\n",
    "- P(k) = precision at position k\n",
    "- rel(k) = 1 if item at position k is relevant, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfefa70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize variables\n",
    "actual = [\n",
    "    [2, 4, 5, 7, 9, 3],\n",
    "    [1, 3, 6, 8],\n",
    "    [4, 3, 1, 10, 11]\n",
    "]\n",
    "predicted =  [1, 2, 3, 4, 5, 6, 8]\n",
    "k = 6\n",
    "Q = len(actual)\n",
    "AP = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd8d8ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query 1:  Average Precision (AP): 0.45\n",
      "Query 2:  Average Precision (AP): 0.54\n",
      "Query 3:  Average Precision (AP): 0.48\n",
      "Mean Average Precision (MAP): 0.49\n"
     ]
    }
   ],
   "source": [
    "for q in range(Q):\n",
    "    actual_set = set(actual[q])\n",
    "    ap_num = 0\n",
    "    for i in range(1, k+1):\n",
    "        predicted_set = set(predicted[:i])\n",
    "        intersection = actual_set.intersection(predicted_set)\n",
    "        precision_at_i = len(intersection) / i\n",
    "        if predicted[i-1] in actual_set:\n",
    "            rel_k = 1\n",
    "        else:\n",
    "            rel_k = 0\n",
    "        ap_num += precision_at_i * rel_k\n",
    "    ap = ap_num / len(actual_set)\n",
    "    print(f\"Query {q+1}:  Average Precision (AP): {ap:.2f}\")\n",
    "    AP.append(ap)\n",
    "MAP = sum(AP) / Q\n",
    "print(f\"Mean Average Precision (MAP): {MAP:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b912fb2a",
   "metadata": {},
   "source": [
    "# NDCG( Normalized Discounted Cumulative Gain)\n",
    "Normalized Discounted Cumulative Gain (NDCG) is a ranking metric that considers both the relevance of items and their positions. Here are the key concepts:\n",
    "\n",
    "1. **Relevance Scores**: Unlike previous metrics that use binary relevance (relevant/not relevant), NDCG can work with graded relevance scores (e.g., 0-5 scale).\n",
    "\n",
    "2. **Position Matters**: Items appearing earlier in the ranking contribute more to the score, with a logarithmic discount applied to later positions.\n",
    "\n",
    "3. **Normalization**: NDCG normalizes the score by comparing it to the ideal ranking, producing values between 0 and 1.\n",
    "\n",
    "**Formulas:**\n",
    "\n",
    "**Cumulative Gain (CG):**\n",
    "$$CG@k = \\sum_{i=1}^{k} rel_i$$\n",
    "\n",
    "**Discounted Cumulative Gain (DCG):**\n",
    "$$DCG@k = \\sum_{i=1}^{k} \\frac{rel_i}{\\log_2(i+1)}$$\n",
    "\n",
    "**Ideal DCG (IDCG):**\n",
    "- Sort all relevant items by their relevance scores in descending order\n",
    "- Calculate DCG for this ideal ranking\n",
    "$$IDCG@k = \\sum_{i=1}^{k} \\frac{rel_i^{ideal}}{\\log_2(i+1)}$$\n",
    "\n",
    "**Normalized DCG (NDCG):**\n",
    "$$NDCG@k = \\frac{DCG@k}{IDCG@k}$$\n",
    "\n",
    "Where:\n",
    "- k = number of top results to consider\n",
    "- rel_i = relevance score of item at position i\n",
    "- NDCG ranges from 0 to 1, where 1 means perfect ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7c6b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG at 1: 0.50\n",
      "NDCG at 2: 0.77\n",
      "NDCG at 3: 0.87\n",
      "NDCG at 4: 0.78\n",
      "NDCG at 5: 0.79\n",
      "NDCG at 6: 0.87\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import log2\n",
    "\n",
    "# Example with graded relevance scores (0-5 scale)\n",
    "relevance_scores =    [2, 4, 3, 0, 1, 2]\n",
    "K = 6\n",
    "dcg = 0\n",
    "idcg = 0\n",
    "ideal_relevance = sorted(relevance_scores, reverse=True)\n",
    "\n",
    "for k in range(1, K+1):\n",
    "    rel_k = relevance_scores[k-1]\n",
    "    ideal_rel_k = ideal_relevance[k-1]\n",
    "\n",
    "    dcg = dcg +  rel_k/log2(k+1)\n",
    "    idcg = idcg + ideal_rel_k/log2(k+1)\n",
    "\n",
    "    ndcg = dcg / idcg if idcg > 0 else 0\n",
    "    print(f\"NDCG at {k}: {ndcg:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f802bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
