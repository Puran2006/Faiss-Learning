{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9f805e0",
   "metadata": {},
   "source": [
    "# HNSW\n",
    "HNSW is one of the best working ANN algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acac22b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 128), (1, 128))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# now define a function to read the fvecs file format of Sift1M dataset\n",
    "def read_fvecs(fp):\n",
    "    a = np.fromfile(fp, dtype='int32')\n",
    "    d = a[0]\n",
    "    return a.reshape(-1, d + 1)[:, 1:].copy().view('float32')\n",
    "\n",
    "# data we will search through\n",
    "wb = read_fvecs('./sift/sift_base.fvecs')  # 1M samples\n",
    "# also get some query vectors to search with\n",
    "xq = read_fvecs('./sift/sift_query.fvecs')\n",
    "# take just one query (there are many in sift_learn.fvecs)\n",
    "xq = xq[0].reshape(1, xq.shape[1])\n",
    "\n",
    "wb.shape, xq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d76617c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "d = wb.shape[1]  # dimension\n",
    "M = 32 # number of Neighbours\n",
    "index = faiss.IndexHNSWFlat(d, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b449c0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nothing is created in the HNSW index\n",
    "index.hnsw.max_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "760abe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels = faiss.vector_to_array(index.hnsw.levels)\n",
    "np.bincount(levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27382999",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.add(wb)\n",
    "# This might be longer than usual as HNSW builds the graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6074445e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.hnsw.max_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c4876b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0, 968746,  30276,    951,     26,      1], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels = faiss.vector_to_array(index.hnsw.levels)\n",
    "np.bincount(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10ceb9",
   "metadata": {},
   "source": [
    "``` array([     0, 968746,  30276,    951,     26,      1], dtype=int64)```\n",
    "```\n",
    "968746 - layer 0\n",
    "30276  - layer 1\n",
    "951    - layer 2\n",
    "26     - layer 3\n",
    "1      - Entry Point\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8851003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118295"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can check the vector that is being used as entry points\n",
    "index.hnsw.entry_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ae0993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets just look at the probabilty function used to assign levels to nodes, from OFFICIAL FAISS GITHUB REPO\n",
    "# THe official is in cpp, we convert to python here for understanding. CPP is more efficient.\n",
    "def set_default_probas(M: int, m_L: float):\n",
    "    nn = 0  # set nearest neighbors count = 0\n",
    "    cum_nneighbor_per_level = []\n",
    "    level = 0  # we start at level 0\n",
    "    assign_probas = []\n",
    "    while True:\n",
    "        # calculate probability for current level\n",
    "        proba = np.exp(-level / m_L) * (1 - np.exp(-1 / m_L))\n",
    "        # once we reach low prob threshold, we've created enough levels\n",
    "        if proba < 1e-9: break\n",
    "        assign_probas.append(proba)\n",
    "        # neighbors is == M on every level except level 0 where == M*2\n",
    "        nn += M*2 if level == 0 else M\n",
    "        cum_nneighbor_per_level.append(nn)\n",
    "        level += 1\n",
    "    return assign_probas, cum_nneighbor_per_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6a9e3",
   "metadata": {},
   "source": [
    "Here we are building two vectors — assign_probas, the probability of insertion at a given layer, and cum_nneighbor_per_level, the cumulative total of nearest neighbors assigned to a vertex at different insertion levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b271ed89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.96875,\n",
       "  0.030273437499999986,\n",
       "  0.0009460449218749991,\n",
       "  2.956390380859371e-05,\n",
       "  9.23871994018553e-07,\n",
       "  2.887099981307982e-08],\n",
       " [64, 96, 128, 160, 192, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets see how many levels are created for M = 32 manually\n",
    "assign_probas, cum_nneighbor_per_level = set_default_probas(32, 1/np.log(32))\n",
    "assign_probas, cum_nneighbor_per_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a168cd",
   "metadata": {},
   "source": [
    "From this, we can see the significantly higher probability of inserting a vector at level 0 than higher levels (although, as we will explain below, the probability is not exactly as defined here). \n",
    "\n",
    "Our assign_probas vector is used by another method called random_level — it is in this function that each vertex is assigned an insertion level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08af3ca",
   "metadata": {},
   "source": [
    "We generate a random float using Numpy’s random number generator rng (initialized below) in f. For each level, we check if f is less than the assigned probability for that level in assign_probas — if so, that is our insertion layer.\n",
    "\n",
    "If f is too high, we subtract the assign_probas value from f and try again for the next level. The result of this logic is that vectors are most likely going to be inserted at level 0. Still, if not, there is a decreasing probability of insertion at ease increment level.\n",
    "\n",
    "Finally, if no levels satisfy the probability condition, we insert the vector at the highest level with return len(assign_probas) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6ef83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def random_level(assign_probas, rng):\n",
    "    r = rng.uniform() # random number between 0 and 1\n",
    "\n",
    "    for level in range(len(assign_probas)):\n",
    "        if r < assign_probas[level]:\n",
    "            return level\n",
    "        \n",
    "        r = r - assign_probas[level]\n",
    "\n",
    "    return len(assign_probas) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e96c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(seed=12345)\n",
    "manual_levels = [random_level(assign_probas, rng) for _ in range(wb.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a89d689c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([968821,  30170,    985,     23,      1], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(manual_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "489fcafe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0, 968746,  30276,    951,     26,      1], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b69df4e",
   "metadata": {},
   "source": [
    "If you compare the manually generated levels and HNSW levels they are almost the same. its actually cool I think"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1664bf9",
   "metadata": {},
   "source": [
    "Now that we’ve explored all there is to explore on the theory behind HNSW and how this is implemented in Faiss — let’s look at the effect of different parameters on our recall, search and build times, and the memory usage of each.\n",
    "\n",
    "Our **efConstruction** value must be set before we construct the index via index.add(xb), but **efSearch** can be set anytime before searching.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e7e1d696",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "efConstruction = 32\n",
    "efSearch = 16\n",
    "M = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5031ca67",
   "metadata": {},
   "source": [
    "### After trying different M, efConstruction and efSearch values, we can set them as follows:\n",
    "- ef construction affects the index building time more than search time\n",
    "- ef search affects the search time more than index building time\n",
    "- M controls the number of neighbors, higher M means better accuracy but more memory and slower search\n",
    "-  We can also increase efConstruction to achieve higher recall at lower M and efSearch values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44fd84c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.hnsw.efConstruction = efConstruction\n",
    "index.add(wb)  # build the index\n",
    "index.hnsw.efSearch = efSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fbc4a0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2 ms ± 19.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "index.search(xq, 5)  # search 5 nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98dc6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss-learn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
